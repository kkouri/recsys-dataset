{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Item relation matrices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (12_899_779, 2)\n",
      "┌──────────┬─────────────────────────────────┐\n",
      "│ session  ┆ events                          │\n",
      "│ ---      ┆ ---                             │\n",
      "│ u32      ┆ list[struct[3]]                 │\n",
      "╞══════════╪═════════════════════════════════╡\n",
      "│ 0        ┆ [{1517085,1659304800025,\"click… │\n",
      "│ 1        ┆ [{424964,1659304800025,\"carts\"… │\n",
      "│ 2        ┆ [{763743,1659304800038,\"clicks… │\n",
      "│ 3        ┆ [{1425967,1659304800095,\"carts… │\n",
      "│ 4        ┆ [{613619,1659304800119,\"clicks… │\n",
      "│ …        ┆ …                               │\n",
      "│ 12899774 ┆ [{33035,1661723968869,\"clicks\"… │\n",
      "│ 12899775 ┆ [{1743151,1661723970935,\"click… │\n",
      "│ 12899776 ┆ [{548599,1661723972537,\"clicks… │\n",
      "│ 12899777 ┆ [{384045,1661723976974,\"clicks… │\n",
      "│ 12899778 ┆ [{561560,1661723983611,\"clicks… │\n",
      "└──────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# define the schema of the dataframe\n",
    "event_schema = pl.Struct({\"aid\": pl.UInt32, \"ts\": pl.UInt64, \"type\": str})\n",
    "df_schema = {\"session\": pl.UInt32, \"events\": pl.List(event_schema)}\n",
    "\n",
    "df = pl.read_ndjson('../data/train.jsonl', schema=df_schema, low_memory=True)\n",
    "\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (216_716_096, 4)\n",
      "┌──────────┬─────────┬────────────┬────────┐\n",
      "│ session  ┆ aid     ┆ ts         ┆ type   │\n",
      "│ ---      ┆ ---     ┆ ---        ┆ ---    │\n",
      "│ u32      ┆ u32     ┆ u32        ┆ str    │\n",
      "╞══════════╪═════════╪════════════╪════════╡\n",
      "│ 0        ┆ 1517085 ┆ 1659304800 ┆ clicks │\n",
      "│ 0        ┆ 1563459 ┆ 1659304904 ┆ clicks │\n",
      "│ 0        ┆ 1309446 ┆ 1659367439 ┆ clicks │\n",
      "│ 0        ┆ 16246   ┆ 1659367719 ┆ clicks │\n",
      "│ 0        ┆ 1781822 ┆ 1659367871 ┆ clicks │\n",
      "│ …        ┆ …       ┆ …          ┆ …      │\n",
      "│ 12899776 ┆ 1737908 ┆ 1661723987 ┆ clicks │\n",
      "│ 12899777 ┆ 384045  ┆ 1661723976 ┆ clicks │\n",
      "│ 12899777 ┆ 384045  ┆ 1661723986 ┆ clicks │\n",
      "│ 12899778 ┆ 561560  ┆ 1661723983 ┆ clicks │\n",
      "│ 12899778 ┆ 32070   ┆ 1661723994 ┆ clicks │\n",
      "└──────────┴─────────┴────────────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "# Sessions\n",
    "exploded_df = (\n",
    "    df\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    # Convert ts to seconds and cast to UInt32 to save memory\n",
    "    .with_columns((pl.col(\"ts\")//1000).cast(pl.UInt32))\n",
    ")\n",
    "\n",
    "print(exploded_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_sub_sessions(with_next_event=True, only_clicks=False, with_row_index=False, limit=None):\n",
    "    \"\"\"\n",
    "    Splits sessions into sub sessions based on time between events or session boundaries.\n",
    "\n",
    "    Args:\n",
    "        with_next_event: whether rows should include consecutive events or not.\n",
    "        only_clicks: only clicks are considered in the sub sessions\n",
    "        with_row_index: add row index to the sub sessions in order to calculate delta time between events\n",
    "        limit: limit the amount of rows to be processed.\n",
    "\n",
    "    Returns: DataFrame of sub sessions\n",
    "    \"\"\"\n",
    "\n",
    "    sub_sessions = exploded_df\n",
    "\n",
    "    if limit is not None:\n",
    "        sub_sessions = exploded_df.limit(limit)\n",
    "    if only_clicks:\n",
    "        sub_sessions = sub_sessions.filter(pl.col(\"type\") == \"clicks\")\n",
    "\n",
    "    sub_sessions = (\n",
    "        sub_sessions\n",
    "        # Convert ts to seconds and cast to UInt32 to save memory\n",
    "        .with_columns((pl.col(\"ts\")//1000).cast(pl.UInt32))\n",
    "        .sort([\"session\", \"ts\"])\n",
    "        .with_columns(\n",
    "            next_session = pl.col(\"session\").shift(-1),\n",
    "            next_aid = pl.col(\"aid\").shift(-1),\n",
    "            next_ts = pl.col(\"ts\").shift(-1),\n",
    "            next_type = pl.col(\"type\").shift(-1),\n",
    "        )\n",
    "        # Row is a sub session boundary if there is existing session boundary or if time between events is more than 30 minutes\n",
    "        .with_columns(\n",
    "            is_session_boundary = ((pl.col(\"session\") != pl.col(\"next_session\")) | (pl.col(\"next_ts\") - pl.col(\"ts\") > 1800)),\n",
    "        )\n",
    "        .with_columns(\n",
    "            sub_session = pl.col(\"is_session_boundary\").cum_sum().cast(pl.UInt32),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Use this when immediate next event is required\n",
    "    # Last event of sub session is always found in the same row as the second last\n",
    "    if with_next_event:\n",
    "        sub_sessions = (\n",
    "            sub_sessions\n",
    "            # Filter out session boundaries. This also removes sub sessions with only 1 event which are not interesting\n",
    "            .filter(pl.col(\"is_session_boundary\").not_())\n",
    "            .drop(\"is_session_boundary\")\n",
    "        )\n",
    "    # Use this when all events are wanted to be found in their own rows and single column\n",
    "    else:\n",
    "        sub_sessions = (\n",
    "            sub_sessions\n",
    "            # Keep each event in their own row\n",
    "            .with_columns(sub_session = pl.when(pl.col(\"is_session_boundary\")).then(pl.col(\"sub_session\") - 1).otherwise(pl.col(\"sub_session\")))\n",
    "            .drop([\"session\", \"next_session\", \"next_aid\", \"next_ts\", \"next_type\", \"is_session_boundary\"])\n",
    "            .filter(pl.col(\"sub_session\").is_null().not_())\n",
    "        )\n",
    "\n",
    "        # Filter out sub session with only one event\n",
    "        multi_event_sub_sessions = (\n",
    "            sub_sessions\n",
    "            .group_by(\"sub_session\")\n",
    "            .agg(pl.len())\n",
    "            .filter(pl.col(\"len\") > 1)\n",
    "            .select(\"sub_session\")\n",
    "        )\n",
    "\n",
    "        sub_sessions = (\n",
    "            sub_sessions\n",
    "            .join(multi_event_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "        )\n",
    "\n",
    "    if with_row_index:\n",
    "        sub_sessions = sub_sessions.with_row_index()\n",
    "\n",
    "    return sub_sessions\n",
    "\n",
    "# sub_sessions = get_sub_sessions()\n",
    "# print(sub_sessions)\n",
    "# print(\"Amount of sub sessions:\", sub_sessions.select(\"sub_session\").n_unique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# Note there should only be used when sub_session are generated with param with_next_event=False\n",
    "# Otherwise some events are lost\n",
    "\n",
    "def get_clicks_of_sub_session(sub_sessions):\n",
    "    return (\n",
    "        sub_sessions\n",
    "        .filter(pl.col(\"type\") == \"clicks\")\n",
    "        .select([\"sub_session\", \"ts\", \"aid\"])\n",
    "        .rename({\"ts\": \"click_ts\", \"aid\": \"click_aid\"})\n",
    "    )\n",
    "\n",
    "def get_carts_of_sub_session(sub_sessions):\n",
    "    return (\n",
    "        sub_sessions\n",
    "        .filter(pl.col(\"type\") == \"carts\")\n",
    "        .select([\"sub_session\", \"ts\", \"aid\"])\n",
    "        .rename({\"ts\": \"cart_ts\", \"aid\": \"cart_aid\"})\n",
    "    )\n",
    "\n",
    "def get_orders_of_sub_session(sub_sessions):\n",
    "    return (\n",
    "        sub_sessions\n",
    "        .filter(pl.col(\"type\") == \"orders\")\n",
    "        .select([\"sub_session\", \"ts\", \"aid\"])\n",
    "        .rename({\"ts\": \"order_ts\", \"aid\": \"order_aid\"})\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Click to click matrix\n",
    "Click to click matrix is defined as the probabilities of other aids being clicked after the previous aid is clicked.\n",
    "Click to click matrix is formed from the sub sessions since there is no point in counting subsequent clicks that are from a user coming back to site after a long time\n",
    "\n",
    "We do multiple variations of the click to click matrix:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next click only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (65_140_992, 3)\n",
      "┌───────────┬────────────────┬─────────────┐\n",
      "│ click_aid ┆ next_click_aid ┆ probability │\n",
      "│ ---       ┆ ---            ┆ ---         │\n",
      "│ u32       ┆ u32            ┆ f32         │\n",
      "╞═══════════╪════════════════╪═════════════╡\n",
      "│ 0         ┆ 0              ┆ 0.096774    │\n",
      "│ 0         ┆ 36236          ┆ 0.032258    │\n",
      "│ 0         ┆ 89130          ┆ 0.032258    │\n",
      "│ 0         ┆ 137543         ┆ 0.032258    │\n",
      "│ 0         ┆ 215649         ┆ 0.032258    │\n",
      "│ …         ┆ …              ┆ …           │\n",
      "│ 1855602   ┆ 1046590        ┆ 0.071429    │\n",
      "│ 1855602   ┆ 1297712        ┆ 0.071429    │\n",
      "│ 1855602   ┆ 1376245        ┆ 0.071429    │\n",
      "│ 1855602   ┆ 1394911        ┆ 0.071429    │\n",
      "│ 1855602   ┆ 1498281        ┆ 0.071429    │\n",
      "└───────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "subsequent_clicks = (\n",
    "    exploded_df\n",
    "    # Convert ts to seconds and cast to UInt32 to save memory\n",
    "    .with_columns((pl.col(\"ts\")//1000).cast(pl.UInt32))\n",
    "    .filter(pl.col(\"type\") == \"clicks\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .with_columns(\n",
    "        next_session = pl.col(\"session\").shift(-3),\n",
    "        next_aid = pl.col(\"aid\").shift(-3),\n",
    "        next_ts = pl.col(\"ts\").shift(-3),\n",
    "    )\n",
    "    .with_columns(\n",
    "        time_between_min = pl.when(pl.col(\"session\") == pl.col(\"next_session\")).then(((pl.col(\"next_ts\") - pl.col(\"ts\")) / 60)).otherwise(None).cast(pl.Float32)\n",
    "    )\n",
    "    # Click has taken less than one hour\n",
    "    .filter(pl.col(\"time_between_min\").is_not_null() & (pl.col(\"time_between_min\") <= 62))\n",
    "    .select([\"aid\", \"next_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same click to click events there are\n",
    "subsequent_clicks_count = (\n",
    "    subsequent_clicks\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    ")\n",
    "\n",
    "# Sum all the clicks for each aid\n",
    "aid_clicks_total_count = (\n",
    "    subsequent_clicks_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being clicked immediately after another item has been clicked\n",
    "click_to_click_matrix = (\n",
    "    subsequent_clicks_count\n",
    "    .join(aid_clicks_total_count, on=\"aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"click_aid\", \"next_aid\": \"next_click_aid\"})\n",
    ")\n",
    "\n",
    "print(click_to_click_matrix)\n",
    "\n",
    "click_to_click_matrix.write_csv(\"./click_to_click_matrix_60_3.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions()\n",
    "\n",
    "# Count how many same click to click events there are\n",
    "subsequent_clicks_count = (\n",
    "    sub_sessions\n",
    "    .filter((pl.col(\"type\") == \"clicks\") & (pl.col(\"next_type\") == \"clicks\"))\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    ")\n",
    "\n",
    "# Sum all the clicks for each aid\n",
    "aid_clicks_total_count = (\n",
    "    subsequent_clicks_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being clicked immediately after another item has been clicked\n",
    "click_to_click_matrix = (\n",
    "    subsequent_clicks_count\n",
    "    .join(aid_clicks_total_count, on=\"aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"click_aid\", \"next_aid\": \"next_click_aid\"})\n",
    ")\n",
    "\n",
    "print(click_to_click_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_click_matrix.select(\"aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_click_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "click_to_click_matrix.write_csv(\"./click_to_click_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next click only. Remove carts and orders from the sub sessions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(only_clicks=True)\n",
    "\n",
    "# Count how many same click to click events there are\n",
    "subsequent_clicks_count = (\n",
    "    sub_sessions\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    ")\n",
    "\n",
    "# Sum all the clicks for each aid\n",
    "aid_clicks_total_count = (\n",
    "    subsequent_clicks_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being clicked immediately after another item has been clicked\n",
    "click_to_click_matrix = (\n",
    "    subsequent_clicks_count\n",
    "    .join(aid_clicks_total_count, on=\"aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"click_aid\", \"next_aid\": \"next_click_aid\"})\n",
    ")\n",
    "\n",
    "print(click_to_click_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_click_matrix.select(\"aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_click_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "click_to_click_matrix.write_csv(\"./click_to_click_matrix_only_clicks.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next two clicks with time decay. Remove carts and orders from the sub sessions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (200_096_794, 3)\n",
      "┌───────────┬─────────────┬─────────┐\n",
      "│ index     ┆ sub_session ┆ aid     │\n",
      "│ ---       ┆ ---         ┆ ---     │\n",
      "│ u32       ┆ u32         ┆ u32     │\n",
      "╞═══════════╪═════════════╪═════════╡\n",
      "│ 0         ┆ 12115466    ┆ 552662  │\n",
      "│ 1         ┆ 12115466    ┆ 871283  │\n",
      "│ 2         ┆ 12115466    ┆ 1436133 │\n",
      "│ 3         ┆ 12115466    ┆ 871283  │\n",
      "│ 4         ┆ 12115466    ┆ 1006139 │\n",
      "│ …         ┆ …           ┆ …       │\n",
      "│ 200096789 ┆ 2372394     ┆ 1283290 │\n",
      "│ 200096790 ┆ 2372394     ┆ 723240  │\n",
      "│ 200096791 ┆ 10679973    ┆ 1828755 │\n",
      "│ 200096792 ┆ 10679973    ┆ 1581840 │\n",
      "│ 200096793 ┆ 10679973    ┆ 63237   │\n",
      "└───────────┴─────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# Read and manipulate data and then save it to csv in order to rest with streaming.\n",
    "# Running out of memory otherwise\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# define the schema of the dataframe\n",
    "event_schema = pl.Struct({\"aid\": pl.UInt32, \"ts\": pl.UInt64, \"type\": str})\n",
    "df_schema = {\"session\": pl.UInt32, \"events\": pl.List(event_schema)}\n",
    "\n",
    "sub_sessions = (\n",
    "    pl.read_ndjson('../data/train.jsonl', schema=df_schema, low_memory=True)\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    # Convert ts to seconds and cast to UInt32 to save memory\n",
    "    .with_columns((pl.col(\"ts\")//1000).cast(pl.UInt32))\n",
    "    .sort([\"session\", \"ts\"])\n",
    "    .with_columns(\n",
    "        next_session = pl.col(\"session\").shift(-1),\n",
    "        next_aid = pl.col(\"aid\").shift(-1),\n",
    "        next_ts = pl.col(\"ts\").shift(-1),\n",
    "        next_type = pl.col(\"type\").shift(-1),\n",
    "    )\n",
    "    # Row is a sub session boundary if there is existing session boundary or if time between events is more than 1 hour\n",
    "    .with_columns(\n",
    "        is_session_boundary = ((pl.col(\"session\") != pl.col(\"next_session\")) | (pl.col(\"next_ts\") - pl.col(\"ts\") > 3600)),\n",
    "    )\n",
    "    .with_columns(\n",
    "        sub_session = pl.col(\"is_session_boundary\").cum_sum().cast(pl.UInt32),\n",
    "    )\n",
    "    # Keep each event in their own row\n",
    "    .with_columns(sub_session = pl.when(pl.col(\"is_session_boundary\")).then(pl.col(\"sub_session\") - 1).otherwise(pl.col(\"sub_session\")))\n",
    "    .drop([\"session\", \"ts\", \"type\", \"next_session\", \"next_aid\", \"next_ts\", \"next_type\", \"is_session_boundary\"])\n",
    "    .filter(pl.col(\"sub_session\").is_null().not_())\n",
    "    # Filter sub session with only one event\n",
    "    .group_by(pl.col(\"sub_session\"))\n",
    "    .agg(pl.col(\"aid\"))\n",
    "    .filter(pl.col(\"aid\").list.len() > 1)\n",
    "    .explode(\"aid\")\n",
    "    .with_row_index()\n",
    ")\n",
    "\n",
    "print(sub_sessions)\n",
    "\n",
    "# Save to csv\n",
    "sub_sessions.write_csv(\"./clicks/clicks_of_sub_sessions_1h.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The system cannot find the file specified. (os error 2): ./clicks/clicks_of_sub_sessions_1h.csv\n\nThis error occurred with the following context stack:\n\t[1] 'csv scan'\n\t[2] 'join left'\n\t[3] 'join'\n\t[4] 'with_columns'\n\t[5] 'filter'\n\t[6] 'with_columns'\n\t[7] 'format!(\"{}\", function).to_lowercase()'\n\t[8] 'group_by'\n\t[9] 'join left'\n\t[10] 'join'\n\t[11] 'with_columns'\n\t[12] 'format!(\"{}\", function).to_lowercase()'\n\t[13] 'sort'\n\t[14] 'format!(\"{}\", function).to_lowercase()'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 44\u001B[0m\n\u001B[0;32m     35\u001B[0m aid_clicks_total_count \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     36\u001B[0m     click_to_click_count\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;241m.\u001B[39mgroup_by(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;241m.\u001B[39magg(pl\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweighted_count\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweighted_total_count\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     39\u001B[0m )\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Calculate the probabilities of items being clicked after another item has been clicked\u001B[39;00m\n\u001B[0;32m     43\u001B[0m click_to_click_matrix \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m---> 44\u001B[0m     \u001B[43mclick_to_click_count\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43maid_clicks_total_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minner\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwith_columns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprobability\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweighted_count\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweighted_total_count\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweighted_count\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweighted_total_count\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnext_aid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrename\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mclick_aid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnext_aid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnext_click_aid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstreaming\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m )\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mprint\u001B[39m(click_to_click_matrix)\n",
      "File \u001B[1;32m~\\.virtualenvs\\recsys-dataset-onFW_Sex\\lib\\site-packages\\polars\\lazyframe\\frame.py:2021\u001B[0m, in \u001B[0;36mLazyFrame.collect\u001B[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, streaming, engine, background, _eager, **_kwargs)\u001B[0m\n\u001B[0;32m   2019\u001B[0m \u001B[38;5;66;03m# Only for testing purposes\u001B[39;00m\n\u001B[0;32m   2020\u001B[0m callback \u001B[38;5;241m=\u001B[39m _kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_opt_callback\u001B[39m\u001B[38;5;124m\"\u001B[39m, callback)\n\u001B[1;32m-> 2021\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap_df(\u001B[43mldf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: The system cannot find the file specified. (os error 2): ./clicks/clicks_of_sub_sessions_1h.csv\n\nThis error occurred with the following context stack:\n\t[1] 'csv scan'\n\t[2] 'join left'\n\t[3] 'join'\n\t[4] 'with_columns'\n\t[5] 'filter'\n\t[6] 'with_columns'\n\t[7] 'format!(\"{}\", function).to_lowercase()'\n\t[8] 'group_by'\n\t[9] 'join left'\n\t[10] 'join'\n\t[11] 'with_columns'\n\t[12] 'format!(\"{}\", function).to_lowercase()'\n\t[13] 'sort'\n\t[14] 'format!(\"{}\", function).to_lowercase()'\n"
     ]
    }
   ],
   "source": [
    "# Run computations in streaming mode in order to not run out of memory\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "n_subsequent_clicks = 7\n",
    "\n",
    "df_schema = {\n",
    "    \"index\": pl.UInt32,\n",
    "    \"sub_session\": pl.UInt32,\n",
    "    \"aid\": pl.UInt32\n",
    "}\n",
    "\n",
    "subsequent_clicks = (\n",
    "    pl.scan_csv(\"./clicks/clicks_of_sub_sessions_1h.csv\", schema=df_schema)\n",
    "    .join(\n",
    "        pl.scan_csv(\"./clicks/clicks_of_sub_sessions_1h.csv\", schema=df_schema).rename({\"index\": \"next_index\", \"aid\": \"next_aid\"}),\n",
    "        on=\"sub_session\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .with_columns(delta_index=(pl.col(\"next_index\") - pl.col(\"index\")))\n",
    "    .filter((pl.col(\"delta_index\") >= 1) & (pl.col(\"delta_index\") <= n_subsequent_clicks))\n",
    "    .with_columns(weight=(1/pl.col(\"delta_index\")).cast(pl.Float32))\n",
    "    .drop([\"index\", \"next_index\", \"sub_session\", \"delta_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each click-click pair\n",
    "click_to_click_count = (\n",
    "    subsequent_clicks\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each click\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_clicks_total_count = (\n",
    "    click_to_click_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.col(\"weighted_count\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the probabilities of items being clicked after another item has been clicked\n",
    "click_to_click_matrix = (\n",
    "    click_to_click_count\n",
    "    .join(aid_clicks_total_count, on=\"aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"click_aid\", \"next_aid\": \"next_click_aid\"})\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(click_to_click_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_click_matrix.select(\"click_aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_click_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "click_to_click_matrix.write_csv(\"./click_to_click_matrix_only_clicks_time_decay_7_1h.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (234_067_458, 3)\n",
      "┌───────────┬────────────────┬─────────────┐\n",
      "│ click_aid ┆ next_click_aid ┆ probability │\n",
      "│ ---       ┆ ---            ┆ ---         │\n",
      "│ u32       ┆ u32            ┆ f32         │\n",
      "╞═══════════╪════════════════╪═════════════╡\n",
      "│ 0         ┆ 0              ┆ 0.047685    │\n",
      "│ 0         ┆ 8136           ┆ 0.005961    │\n",
      "│ 0         ┆ 13759          ┆ 0.011921    │\n",
      "│ 0         ┆ 29217          ┆ 0.00298     │\n",
      "│ 0         ┆ 31465          ┆ 0.00298     │\n",
      "│ …         ┆ …              ┆ …           │\n",
      "│ 1855602   ┆ 1739200        ┆ 0.007009    │\n",
      "│ 1855602   ┆ 1762441        ┆ 0.005607    │\n",
      "│ 1855602   ┆ 1768521        ┆ 0.005607    │\n",
      "│ 1855602   ┆ 1783511        ┆ 0.042056    │\n",
      "│ 1855602   ┆ 1855602        ┆ 0.035047    │\n",
      "└───────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Run computations in streaming mode in order to not run out of memory\n",
    "\n",
    "# median_ts = (\n",
    "#     exploded_df\n",
    "#     .select(\"ts\")\n",
    "#     .median()\n",
    "#     .item()\n",
    "# )\n",
    "# print(median_ts)\n",
    "\n",
    "subsequent_clicks_of_sessions = (\n",
    "    exploded_df\n",
    "    # Take last half of events this is around 2 weeks\n",
    "    # .filter((pl.col(\"type\") == \"clicks\") & (pl.col(\"ts\") > median_ts))\n",
    "    .filter(pl.col(\"type\") == \"clicks\")\n",
    "    .drop(\"type\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .with_columns(\n",
    "        next_session=pl.col(\"session\").shift(-1),\n",
    "        next_aid=pl.col(\"aid\").shift(-1),\n",
    "        next_ts=pl.col(\"ts\").shift(-1),\n",
    "        weight=pl.lit(1).cast(pl.Float32)\n",
    "    )\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "click_pairs = subsequent_clicks_of_sessions.collect(streaming=True)\n",
    "\n",
    "# more than 1 click ahead\n",
    "n_clicks_ahead = 5\n",
    "for i in range(2, n_clicks_ahead+1):\n",
    "    new_click_pairs = (\n",
    "        # take the original clicks of sessions\n",
    "        click_pairs\n",
    "        .with_columns(\n",
    "            next_session=pl.col(\"session\").shift(-i),\n",
    "            next_aid=pl.col(\"aid\").shift(-i),\n",
    "            next_ts=pl.col(\"ts\").shift(-i),\n",
    "            weight=pl.lit(1/i).cast(pl.Float32)\n",
    "        )\n",
    "        # same session and less than 45 minutes apart\n",
    "        .filter(pl.col(\"next_session\").is_not_null() & (pl.col(\"session\") == pl.col(\"next_session\")) & (pl.col(\"next_ts\") - pl.col(\"ts\") < 45*60))\n",
    "        .lazy()\n",
    "    )\n",
    "    subsequent_clicks_of_sessions = pl.concat([\n",
    "        subsequent_clicks_of_sessions,\n",
    "        new_click_pairs\n",
    "    ])\n",
    "\n",
    "# print(subsequent_clicks_of_sessions.collect(streaming=True))\n",
    "\n",
    "\n",
    "# Sum all the weights for each click-click pair\n",
    "click_to_click_count = (\n",
    "    subsequent_clicks_of_sessions\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each click\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_clicks_total_count = (\n",
    "    click_to_click_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.col(\"weighted_count\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the probabilities of items being clicked after another item has been clicked\n",
    "click_to_click_matrix = (\n",
    "    click_to_click_count\n",
    "    .join(aid_clicks_total_count, on=\"aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"click_aid\", \"next_aid\": \"next_click_aid\"})\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(click_to_click_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "click_to_click_matrix.write_csv(\"./click-to-click-matrix_only-clicks_5-subsequent-clicks-45-min-time-decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Click to cart matrix\n",
    "Click to cart matrix is defined as the probabilities of other aids being added to cart in the same sub session after an aid is clicked.\n",
    "Click to cart matrix is formed from the sub sessions since the sub session should show clear intent of the user to buy items they click."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get clicks and carts of sub sessions\n",
    "clicks_of_sub_sessions = get_clicks_of_sub_session(sub_sessions)\n",
    "\n",
    "carts_of_sub_sessions = get_carts_of_sub_session(sub_sessions)\n",
    "\n",
    "carts_after_clicks_in_sub_sessions = (\n",
    "    carts_of_sub_sessions\n",
    "    # Combine clicks and carts of sub sessions\n",
    "    .join(clicks_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only carts that happened after clicks\n",
    "    .filter(pl.col(\"click_ts\") < pl.col(\"cart_ts\"))\n",
    "    .select([\"sub_session\", \"click_aid\", \"cart_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same click to cart events there are\n",
    "click_to_cart_count = (\n",
    "    carts_after_clicks_in_sub_sessions\n",
    "    .group_by([\"click_aid\", \"cart_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the clicks for each aid\n",
    "aid_clicks_total_count = (\n",
    "    click_to_cart_count\n",
    "    .group_by(\"click_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being added to cart after another item has been clicked\n",
    "click_to_cart_matrix = (\n",
    "    click_to_cart_count\n",
    "    .join(aid_clicks_total_count, on=\"click_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"click_aid\", \"cart_aid\"])\n",
    ")\n",
    "\n",
    "print(click_to_cart_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_cart_matrix.select(\"click_aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_cart_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "click_to_cart_matrix.write_csv(\"./click_to_cart_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Click to cart with time decay. Carts happening later in the session count less into the probability than carts that are closer to the click.\n",
    "The first cart relative to the clicked aid has weight of 1, second has weight of 1/2, third 1/3, etc..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = (\n",
    "    get_sub_sessions(with_next_event=False)\n",
    "    # Filter out orders\n",
    "    .filter(pl.col(\"type\") != \"orders\")\n",
    "    .with_columns(index=(pl.col(\"sub_session\") + (pl.col(\"type\") == \"carts\").cum_sum()))\n",
    ")\n",
    "\n",
    "# Get clicks and carts of sub sessions\n",
    "clicks_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"clicks\")\n",
    "    .rename({\"ts\": \"click_ts\", \"aid\": \"click_aid\", \"index\": \"click_index\"})\n",
    ")\n",
    "\n",
    "carts_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"carts\")\n",
    "    .rename({\"ts\": \"cart_ts\", \"aid\": \"cart_aid\", \"index\": \"cart_index\"})\n",
    ")\n",
    "\n",
    "carts_after_clicks_in_sub_sessions = (\n",
    "    carts_of_sub_sessions\n",
    "    # Combine clicks and carts of sub sessions\n",
    "    .join(clicks_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only carts that happened after clicks\n",
    "    .filter(pl.col(\"click_ts\") < pl.col(\"cart_ts\"))\n",
    "    .select([\"sub_session\", \"click_aid\", \"cart_aid\", \"click_index\", \"cart_index\"])\n",
    "    # Weight the click-to-cart relation based in index. Next cart has weight 1/1, second 1/2, third 1/3, etc...\n",
    "    .with_columns(weight=(1/(pl.col(\"cart_index\")-pl.col(\"click_index\"))).cast(pl.Float32))\n",
    "    .drop([\"click_index\", \"cart_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each click-cart pair\n",
    "click_to_cart_count = (\n",
    "    carts_after_clicks_in_sub_sessions\n",
    "    .group_by([\"click_aid\", \"cart_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each click\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_clicks_total_count = (\n",
    "    carts_after_clicks_in_sub_sessions\n",
    "    .group_by(\"click_aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being added to cart after another item has been clicked\n",
    "click_to_cart_matrix = (\n",
    "    click_to_cart_count\n",
    "    .join(aid_clicks_total_count, on=\"click_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"click_aid\", \"cart_aid\"])\n",
    ")\n",
    "\n",
    "print(click_to_cart_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_cart_matrix.select(\"click_aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_cart_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "click_to_cart_matrix.write_csv(\"./click_to_cart_matrix_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Click to order matrix\n",
    "Click to order matrix is defined as the probabilities of other aids being ordered in the same sub session after an aid is clicked.\n",
    "Click to order matrix is formed from the sub sessions since the sub session should show clear intent of the user to buy items they click."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get clicks and orders of sub sessions\n",
    "clicks_of_sub_sessions = get_clicks_of_sub_session(sub_sessions)\n",
    "\n",
    "orders_of_sub_sessions = get_orders_of_sub_session(sub_sessions)\n",
    "\n",
    "orders_after_clicks_in_sub_sessions = (\n",
    "    orders_of_sub_sessions\n",
    "    # Combine clicks and orders of sub sessions\n",
    "    .join(clicks_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only orders that happened after clicks\n",
    "    .filter(pl.col(\"click_ts\") < pl.col(\"order_ts\"))\n",
    "    .select([\"sub_session\", \"click_aid\", \"order_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same click to order events there are\n",
    "click_to_order_count = (\n",
    "    orders_after_clicks_in_sub_sessions\n",
    "    .group_by([\"click_aid\", \"order_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the clicks for each aid\n",
    "aid_clicks_total_count = (\n",
    "    click_to_order_count\n",
    "    .group_by(\"click_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being ordered after another item has been clicked\n",
    "click_to_order_matrix = (\n",
    "    click_to_order_count\n",
    "    .join(aid_clicks_total_count, on=\"click_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"click_aid\", \"order_aid\"])\n",
    ")\n",
    "\n",
    "print(click_to_order_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_order_matrix.select(\"click_aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "click_to_order_matrix.write_csv(\"./click_to_order_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Click to order with time decay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = (\n",
    "    get_sub_sessions(with_next_event=False)\n",
    "    # Filter out carts\n",
    "    .filter(pl.col(\"type\") != \"carts\")\n",
    "    .with_columns(index=(pl.col(\"sub_session\") + (pl.col(\"type\") == \"orders\").cum_sum()))\n",
    ")\n",
    "\n",
    "# Get clicks and orders of sub sessions\n",
    "clicks_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"clicks\")\n",
    "    .rename({\"ts\": \"click_ts\", \"aid\": \"click_aid\", \"index\": \"click_index\"})\n",
    ")\n",
    "\n",
    "orders_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"orders\")\n",
    "    .rename({\"ts\": \"order_ts\", \"aid\": \"order_aid\", \"index\": \"order_index\"})\n",
    ")\n",
    "\n",
    "orders_after_clicks_in_sub_sessions = (\n",
    "    orders_of_sub_sessions\n",
    "    # Combine clicks and orders of sub sessions\n",
    "    .join(clicks_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only orders that happened after clicks\n",
    "    .filter(pl.col(\"click_ts\") < pl.col(\"order_ts\"))\n",
    "    .select([\"sub_session\", \"click_aid\", \"order_aid\", \"click_index\", \"order_index\"])\n",
    "    # Weight the click-to-order relation based in index. Next order has weight 1/1, second 1/2, third 1/3, etc...\n",
    "    .with_columns(weight=(1/(pl.col(\"order_index\")-pl.col(\"click_index\"))).cast(pl.Float32))\n",
    "    .drop([\"click_index\", \"order_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each click-order pair\n",
    "click_to_order_count = (\n",
    "    orders_after_clicks_in_sub_sessions\n",
    "    .group_by([\"click_aid\", \"order_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each click\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_clicks_total_count = (\n",
    "    orders_after_clicks_in_sub_sessions\n",
    "    .group_by(\"click_aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being added to order after another item has been clicked\n",
    "click_to_order_matrix = (\n",
    "    click_to_order_count\n",
    "    .join(aid_clicks_total_count, on=\"click_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"click_aid\", \"order_aid\"])\n",
    ")\n",
    "\n",
    "print(click_to_order_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", click_to_order_matrix.select(\"click_aid\").n_unique())\n",
    "print(\"Total probability:\", click_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "click_to_order_matrix.write_csv(\"./click_to_order_matrix_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "click to order whole sessions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (12_260_282, 3)\n",
      "┌─────────┬──────────┬─────────────┐\n",
      "│ aid     ┆ next_aid ┆ probability │\n",
      "│ ---     ┆ ---      ┆ ---         │\n",
      "│ u32     ┆ u32      ┆ f32         │\n",
      "╞═════════╪══════════╪═════════════╡\n",
      "│ 0       ┆ 53946    ┆ 0.043478    │\n",
      "│ 0       ┆ 82031    ┆ 0.043478    │\n",
      "│ 0       ┆ 312330   ┆ 0.086957    │\n",
      "│ 0       ┆ 524473   ┆ 0.043478    │\n",
      "│ 0       ┆ 532042   ┆ 0.086957    │\n",
      "│ …       ┆ …        ┆ …           │\n",
      "│ 1855601 ┆ 1580259  ┆ 0.041667    │\n",
      "│ 1855601 ┆ 1708564  ┆ 0.041667    │\n",
      "│ 1855601 ┆ 1712873  ┆ 0.041667    │\n",
      "│ 1855601 ┆ 1716876  ┆ 0.041667    │\n",
      "│ 1855601 ┆ 1749320  ┆ 0.041667    │\n",
      "└─────────┴──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# click to order only next order\n",
    "\n",
    "clicks_and_orders_df = (\n",
    "    df\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    # Convert ts to seconds and cast to UInt32 to save memory\n",
    "    .with_columns((pl.col(\"ts\")//1000).cast(pl.UInt32))\n",
    "    .filter(pl.col(\"type\") != \"carts\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .with_row_index()\n",
    "    # Make groups based on types and then make sure that session boundaries are not crossed\n",
    "    .with_columns(group=(pl.col(\"type\") == \"orders\").rle_id().cast(pl.UInt32))\n",
    "    .with_columns(group=pl.struct(\"session\", \"group\").rle_id())\n",
    "    .group_by([\"session\", \"group\"])\n",
    "    .agg(pl.col(\"ts\").max().alias(\"max_ts\"), pl.col(\"index\").max().alias(\"max_index\"), pl.col(\"aid\"), pl.col(\"ts\"), pl.col(\"type\"), pl.col(\"index\"))\n",
    "    .explode([\"aid\", \"ts\", \"type\", \"index\"])\n",
    "    # Filter clicks of last 1 hour\n",
    "    .filter(pl.when(pl.col(\"type\") == \"clicks\").then(pl.col(\"max_ts\") - pl.col(\"ts\") < 1*60*60).otherwise(True))\n",
    "    # Get last 10 clicks\n",
    "    .filter(pl.when(pl.col(\"type\") == \"clicks\").then(pl.col(\"max_index\") - pl.col(\"index\") < 5).otherwise(True))\n",
    "    .drop([\"ts\", \"max_ts\", \"index\", \"max_index\"])\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "next_event = (\n",
    "     clicks_and_orders_df\n",
    "     .rename({\"group\": \"next_group\", \"aid\": \"next_aid\", \"type\": \"next_type\" })\n",
    ")\n",
    "\n",
    "click_to_order = (\n",
    "    clicks_and_orders_df\n",
    "    .join(next_event, on=\"session\", how=\"inner\")\n",
    "    .filter((pl.col(\"group\") == pl.col(\"next_group\") - 1) & (pl.col(\"type\") == \"clicks\") & (pl.col(\"next_type\") == \"orders\"))\n",
    "    .drop([\"type\", \"next_type\", \"group\", \"next_group\"])\n",
    ")\n",
    "\n",
    "click_to_order_count = (\n",
    "    click_to_order\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    ")\n",
    "\n",
    "click_aid_total_count = (\n",
    "    click_to_order_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    ")\n",
    "\n",
    "click_to_order_matrix_df = (\n",
    "    click_to_order_count\n",
    "    .join(click_aid_total_count, on=\"aid\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"count\") / pl.col(\"total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(click_to_order_matrix_df)\n",
    "\n",
    "# save to csv\n",
    "click_to_order_matrix_df.write_csv(\"./click_to_order_matrix_whole_sessions_only_next_order_last_1h_max_5.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (143_112_255, 5)\n",
      "┌──────────┬─────────┬───────────────┬──────────┬───────────────┐\n",
      "│ session  ┆ aid     ┆ ts            ┆ next_aid ┆ next_ts       │\n",
      "│ ---      ┆ ---     ┆ ---           ┆ ---      ┆ ---           │\n",
      "│ u32      ┆ u32     ┆ u64           ┆ u32      ┆ u64           │\n",
      "╞══════════╪═════════╪═══════════════╪══════════╪═══════════════╡\n",
      "│ 0        ┆ 1517085 ┆ 1659304800025 ┆ 305831   ┆ 1659370027105 │\n",
      "│ 0        ┆ 1563459 ┆ 1659304904511 ┆ 305831   ┆ 1659370027105 │\n",
      "│ 0        ┆ 1309446 ┆ 1659367439426 ┆ 305831   ┆ 1659370027105 │\n",
      "│ 0        ┆ 16246   ┆ 1659367719997 ┆ 305831   ┆ 1659370027105 │\n",
      "│ 0        ┆ 1781822 ┆ 1659367871344 ┆ 305831   ┆ 1659370027105 │\n",
      "│ …        ┆ …       ┆ …             ┆ …        ┆ …             │\n",
      "│ 12899525 ┆ 1599360 ┆ 1661723859676 ┆ 996393   ┆ 1661723996896 │\n",
      "│ 12899525 ┆ 127479  ┆ 1661723746653 ┆ 956231   ┆ 1661723996896 │\n",
      "│ 12899525 ┆ 996393  ┆ 1661723809311 ┆ 956231   ┆ 1661723996896 │\n",
      "│ 12899525 ┆ 127479  ┆ 1661723833093 ┆ 956231   ┆ 1661723996896 │\n",
      "│ 12899525 ┆ 1599360 ┆ 1661723859676 ┆ 956231   ┆ 1661723996896 │\n",
      "└──────────┴─────────┴───────────────┴──────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# click to order only next order\n",
    "\n",
    "clicks_and_orders_df = (\n",
    "    df\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    .filter(pl.col(\"type\") != \"carts\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .drop(\"ts\")\n",
    "    # Make groups based on types and then make sure that session boundaries are not crossed\n",
    "    .with_columns(group=(pl.col(\"type\") == \"orders\").rle_id().cast(pl.UInt32))\n",
    "    .with_columns(group=pl.struct(\"session\", \"group\").rle_id())\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "click_to_order = (\n",
    "    clicks_and_orders_df\n",
    "    .join(clicks_and_orders_df.rename({\"group\": \"next_group\", \"aid\": \"next_aid\", \"type\": \"next_type\" }), on=\"session\", how=\"inner\")\n",
    "    .filter((pl.col(\"group\") == pl.col(\"next_group\") - 1) & (pl.col(\"type\") == \"clicks\") & (pl.col(\"next_type\") == \"orders\"))\n",
    "    .drop([\"type\", \"next_type\", \"group\", \"next_group\"])\n",
    ")\n",
    "\n",
    "click_to_order_count = (\n",
    "    click_to_order\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    # time decay\n",
    "    # .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    "    # no time decay\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    # .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "click_aid_total_count = (\n",
    "    click_to_order_count\n",
    "    .group_by(\"aid\")\n",
    "    # time decay\n",
    "    # .agg(pl.col(\"weighted_count\").sum().alias(\"weighted_total_count\"))\n",
    "    # no time decay\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    # .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "click_to_order_matrix_df = (\n",
    "    click_to_order_count\n",
    "    .join(click_aid_total_count, on=\"aid\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"count\") / pl.col(\"total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(click_to_order_matrix_df)\n",
    "\n",
    "# save to csv\n",
    "click_to_order_matrix_df.write_csv(\"./click_to_order_matrix_whole_sessions_only_next_order.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cart to click matrix\n",
    "Cart to click matrix is defined as the probabilities of other aids being clicked immediately after an aid is added to cart."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get carts and clicks of sub sessions\n",
    "carts_of_sub_sessions = get_carts_of_sub_session(sub_sessions)\n",
    "\n",
    "clicks_of_sub_sessions = get_clicks_of_sub_session(sub_sessions)\n",
    "\n",
    "clicks_after_carts_in_sub_sessions = (\n",
    "    clicks_of_sub_sessions\n",
    "    # Combine carts and clicks of sub sessions\n",
    "    .join(carts_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only clicks that happened after carts\n",
    "    .filter(pl.col(\"cart_ts\") < pl.col(\"click_ts\"))\n",
    "    .select([\"sub_session\", \"cart_aid\", \"click_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same cart to click events there are\n",
    "cart_to_click_count = (\n",
    "    clicks_after_carts_in_sub_sessions\n",
    "    .group_by([\"cart_aid\", \"click_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the carts for each aid\n",
    "aid_carts_total_count = (\n",
    "    cart_to_click_count\n",
    "    .group_by(\"cart_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being clicked immediately after another item has been added to cart\n",
    "cart_to_click_matrix = (\n",
    "    cart_to_click_count\n",
    "    .join(aid_carts_total_count, on=\"cart_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"cart_aid\", \"click_aid\"])\n",
    ")\n",
    "\n",
    "print(cart_to_click_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", cart_to_click_matrix.select(\"cart_aid\").n_unique())\n",
    "print(\"Total probability:\", cart_to_click_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "cart_to_click_matrix.write_csv(\"./cart_to_click_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cart or order to click matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (15_102_363, 4)\n",
      "┌─────────┬────────┬──────────┬────────┐\n",
      "│ aid     ┆ type   ┆ next_aid ┆ weight │\n",
      "│ ---     ┆ ---    ┆ ---      ┆ ---    │\n",
      "│ u32     ┆ str    ┆ u32      ┆ f32    │\n",
      "╞═════════╪════════╪══════════╪════════╡\n",
      "│ 461689  ┆ orders ┆ 362233   ┆ 1.0    │\n",
      "│ 789245  ┆ carts  ┆ 366890   ┆ 1.0    │\n",
      "│ 974651  ┆ carts  ┆ 974651   ┆ 1.0    │\n",
      "│ 1521766 ┆ carts  ┆ 661144   ┆ 1.0    │\n",
      "│ 1760145 ┆ carts  ┆ 1639229  ┆ 1.0    │\n",
      "│ …       ┆ …      ┆ …        ┆ …      │\n",
      "│ 468148  ┆ carts  ┆ 468148   ┆ 1.0    │\n",
      "│ 1126169 ┆ carts  ┆ 1126169  ┆ 1.0    │\n",
      "│ 1379999 ┆ carts  ┆ 992635   ┆ 1.0    │\n",
      "│ 1677695 ┆ carts  ┆ 1677695  ┆ 1.0    │\n",
      "│ 573530  ┆ carts  ┆ 203778   ┆ 1.0    │\n",
      "└─────────┴────────┴──────────┴────────┘\n",
      "shape: (10_153_295, 3)\n",
      "┌─────────┬──────────┬────────────────┐\n",
      "│ aid     ┆ next_aid ┆ weighted_count │\n",
      "│ ---     ┆ ---      ┆ ---            │\n",
      "│ u32     ┆ u32      ┆ f32            │\n",
      "╞═════════╪══════════╪════════════════╡\n",
      "│ 1148621 ┆ 1588038  ┆ 1.0            │\n",
      "│ 380509  ┆ 740194   ┆ 1.0            │\n",
      "│ 1015938 ┆ 261450   ┆ 1.0            │\n",
      "│ 336649  ┆ 610733   ┆ 1.0            │\n",
      "│ 1025223 ┆ 915510   ┆ 1.0            │\n",
      "│ …       ┆ …        ┆ …              │\n",
      "│ 232197  ┆ 915499   ┆ 1.0            │\n",
      "│ 190286  ┆ 459101   ┆ 1.0            │\n",
      "│ 792725  ┆ 1356527  ┆ 1.0            │\n",
      "│ 985535  ┆ 547381   ┆ 1.0            │\n",
      "│ 1412827 ┆ 102466   ┆ 1.0            │\n",
      "└─────────┴──────────┴────────────────┘\n",
      "shape: (1_191_776, 2)\n",
      "┌─────────┬──────────────────────┐\n",
      "│ aid     ┆ weighted_total_count │\n",
      "│ ---     ┆ ---                  │\n",
      "│ u32     ┆ f32                  │\n",
      "╞═════════╪══════════════════════╡\n",
      "│ 1828363 ┆ 1.0                  │\n",
      "│ 1840011 ┆ 24.0                 │\n",
      "│ 346534  ┆ 5.0                  │\n",
      "│ 1670890 ┆ 16.0                 │\n",
      "│ 1667714 ┆ 2.0                  │\n",
      "│ …       ┆ …                    │\n",
      "│ 439366  ┆ 1.0                  │\n",
      "│ 1181034 ┆ 11.0                 │\n",
      "│ 1250287 ┆ 3.0                  │\n",
      "│ 121092  ┆ 1.0                  │\n",
      "│ 1854675 ┆ 1.0                  │\n",
      "└─────────┴──────────────────────┘\n",
      "shape: (10_153_295, 3)\n",
      "┌───────────────────┬────────────────┬─────────────┐\n",
      "│ cart_or_order_aid ┆ next_click_aid ┆ probability │\n",
      "│ ---               ┆ ---            ┆ ---         │\n",
      "│ u32               ┆ u32            ┆ f32         │\n",
      "╞═══════════════════╪════════════════╪═════════════╡\n",
      "│ 3                 ┆ 3              ┆ 0.536585    │\n",
      "│ 3                 ┆ 16778          ┆ 0.006098    │\n",
      "│ 3                 ┆ 24318          ┆ 0.006098    │\n",
      "│ 3                 ┆ 46596          ┆ 0.006098    │\n",
      "│ 3                 ┆ 47546          ┆ 0.006098    │\n",
      "│ …                 ┆ …              ┆ …           │\n",
      "│ 1855600           ┆ 1792795        ┆ 0.166667    │\n",
      "│ 1855601           ┆ 8377           ┆ 0.25        │\n",
      "│ 1855601           ┆ 878643         ┆ 0.25        │\n",
      "│ 1855601           ┆ 1031129        ┆ 0.25        │\n",
      "│ 1855601           ┆ 1636201        ┆ 0.25        │\n",
      "└───────────────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Run computations in streaming mode in order to not run out of memory\n",
    "\n",
    "subsequent_events_of_sessions = (\n",
    "    exploded_df\n",
    "    # Take last half of events this is around 2 weeks\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .drop([\"ts\"])\n",
    "    .with_columns(\n",
    "        next_session=pl.col(\"session\").shift(-1),\n",
    "        next_aid=pl.col(\"aid\").shift(-1),\n",
    "        # next_ts=pl.col(\"ts\").shift(-1),\n",
    "        next_type=pl.col(\"type\").shift(-1),\n",
    "        weight=pl.lit(1).cast(pl.Float32)\n",
    "    )\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "# event_pairs = subsequent_events_of_sessions.collect(streaming=True)\n",
    "# print(event_pairs)\n",
    "\n",
    "cart_or_order_to_click = (\n",
    "    subsequent_events_of_sessions\n",
    "    .filter(\n",
    "        pl.col(\"next_session\").is_not_null()\n",
    "        & (pl.col(\"session\") == pl.col(\"next_session\"))\n",
    "        & (pl.col(\"type\") != \"clicks\")\n",
    "        & (pl.col(\"next_type\") == \"clicks\")\n",
    "    )\n",
    "    .drop([\"session\", \"next_session\", \"next_type\"])\n",
    ")\n",
    "\n",
    "# print(cart_or_order_to_click.collect(streaming=True))\n",
    "\n",
    "# Sum all the weights for each click-click pair\n",
    "cart_or_order_to_click_count = (\n",
    "    cart_or_order_to_click\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# print(cart_or_order_to_click_count.collect(streaming=True))\n",
    "\n",
    "# Count total weight for each click\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_cart_or_order_to_click_total_count = (\n",
    "    cart_or_order_to_click_count\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.col(\"weighted_count\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "# print(aid_cart_or_order_to_click_total_count.collect(streaming=True))\n",
    "\n",
    "# Calculate the probabilities of items being clicked after another item has been clicked\n",
    "cart_or_order_to_click_matrix = (\n",
    "    cart_or_order_to_click_count\n",
    "    .join(aid_cart_or_order_to_click_total_count, on=\"aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"cart_or_order_aid\", \"next_aid\": \"next_click_aid\"})\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(cart_or_order_to_click_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "cart_or_order_to_click_matrix.write_csv(\"./cart_or_order_to_click_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cart to cart matrix\n",
    "Cart to cart matrix is defined as the probabilities of other aids being added to cart later in the same sub session where an aid is added to cart."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get carts of sub sessions\n",
    "carts_of_sub_sessions = get_carts_of_sub_session(sub_sessions)\n",
    "\n",
    "next_carts_of_sub_sessions = (\n",
    "    carts_of_sub_sessions\n",
    "    .rename({\"cart_ts\": \"next_cart_ts\", \"cart_aid\": \"next_cart_aid\"})\n",
    ")\n",
    "\n",
    "# Find subsequent carts in the same sub session\n",
    "subsequent_carts = (\n",
    "    carts_of_sub_sessions\n",
    "    .join(next_carts_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    .filter(pl.col(\"cart_ts\") < pl.col(\"next_cart_ts\"))\n",
    "    .select([\"sub_session\", \"cart_aid\", \"next_cart_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same cart to cart events there are\n",
    "subsequent_carts_count = (\n",
    "    subsequent_carts\n",
    "    .group_by([\"cart_aid\", \"next_cart_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the carts for each aid\n",
    "aid_carts_total_count = (\n",
    "    subsequent_carts_count\n",
    "    .group_by(\"cart_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being added to cart after another item has been added to cart\n",
    "cart_to_cart_matrix = (\n",
    "    subsequent_carts_count\n",
    "    .join(aid_carts_total_count, on=\"cart_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"cart_aid\", \"next_cart_aid\"])\n",
    ")\n",
    "\n",
    "print(cart_to_cart_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", cart_to_cart_matrix.select(\"cart_aid\").n_unique())\n",
    "print(\"Total probability:\", cart_to_cart_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "cart_to_cart_matrix.write_csv(\"./cart_to_cart_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cart to cart with time decay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "carts_of_sub_sessions = (\n",
    "    get_sub_sessions(with_next_event=False)\n",
    "    .filter(pl.col(\"type\") == \"carts\")\n",
    "    .with_row_index()\n",
    "    .rename({\"ts\": \"cart_ts\", \"aid\": \"cart_aid\", \"index\": \"cart_index\"})\n",
    ")\n",
    "\n",
    "next_carts_of_sub_sessions = (\n",
    "    carts_of_sub_sessions\n",
    "    .rename({\"cart_ts\": \"next_cart_ts\", \"cart_aid\": \"next_cart_aid\", \"cart_index\": \"next_cart_index\"})\n",
    ")\n",
    "\n",
    "# Find subsequent carts in the same sub session\n",
    "subsequent_carts = (\n",
    "    carts_of_sub_sessions\n",
    "    .join(next_carts_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    .filter(pl.col(\"cart_ts\") < pl.col(\"next_cart_ts\"))\n",
    "    .select([\"sub_session\", \"cart_aid\", \"next_cart_aid\", \"cart_index\", \"next_cart_index\"])\n",
    "    # Weight the cart-to-cart relation based in index. Next cart has weight 1/1, second 1/2, third 1/3, etc...\n",
    "    .with_columns(weight=(1/(pl.col(\"next_cart_index\")-pl.col(\"cart_index\"))).cast(pl.Float32))\n",
    "    .drop([\"cart_index\", \"next_cart_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each cart-cart pair\n",
    "cart_to_cart_count = (\n",
    "    subsequent_carts\n",
    "    .group_by([\"cart_aid\", \"next_cart_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each cart\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_carts_total_count = (\n",
    "    subsequent_carts\n",
    "    .group_by(\"cart_aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being added to cart after another item has been clicked\n",
    "cart_to_cart_matrix = (\n",
    "    cart_to_cart_count\n",
    "    .join(aid_carts_total_count, on=\"cart_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"cart_aid\", \"next_cart_aid\"])\n",
    ")\n",
    "\n",
    "print(cart_to_cart_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", cart_to_cart_matrix.select(\"cart_aid\").n_unique())\n",
    "print(\"Total probability:\", cart_to_cart_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "cart_to_cart_matrix.write_csv(\"./cart_to_cart_matrix_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cart to order matrix\n",
    "Cart to order matrix is defined as the probabilities of other aids being ordered later in the same sub session where an aid is added to cart."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get carts and orders of sub sessions\n",
    "carts_of_sub_sessions = get_carts_of_sub_session(sub_sessions)\n",
    "\n",
    "orders_of_sub_sessions = get_orders_of_sub_session(sub_sessions)\n",
    "\n",
    "orders_after_carts_in_sub_sessions = (\n",
    "    orders_of_sub_sessions\n",
    "    # Combine carts and orders of sub sessions\n",
    "    .join(carts_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only orders that happened after carts\n",
    "    .filter(pl.col(\"cart_ts\") < pl.col(\"order_ts\"))\n",
    "    .select([\"sub_session\", \"cart_aid\", \"order_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same cart to order events there are\n",
    "cart_to_order_count = (\n",
    "    orders_after_carts_in_sub_sessions\n",
    "    .group_by([\"cart_aid\", \"order_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the carts for each aid\n",
    "aid_carts_total_count = (\n",
    "    cart_to_order_count\n",
    "    .group_by(\"cart_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being ordered after another item has been added to cart\n",
    "cart_to_order_matrix = (\n",
    "    cart_to_order_count\n",
    "    .join(aid_carts_total_count, on=\"cart_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"cart_aid\", \"order_aid\"])\n",
    ")\n",
    "\n",
    "print(cart_to_order_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", cart_to_order_matrix.select(\"cart_aid\").n_unique())\n",
    "print(\"Total probability:\", cart_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "cart_to_order_matrix.write_csv(\"./cart_to_order_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cart to order with time decay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = (\n",
    "    get_sub_sessions(with_next_event=False)\n",
    "    # Filter out clicks\n",
    "    .filter(pl.col(\"type\") != \"clicks\")\n",
    "    .with_columns(index=(pl.col(\"sub_session\") + (pl.col(\"type\") == \"orders\").cum_sum()))\n",
    ")\n",
    "\n",
    "# Get clicks and orders of sub sessions\n",
    "carts_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"carts\")\n",
    "    .rename({\"ts\": \"cart_ts\", \"aid\": \"cart_aid\", \"index\": \"cart_index\"})\n",
    ")\n",
    "\n",
    "orders_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"orders\")\n",
    "    .rename({\"ts\": \"order_ts\", \"aid\": \"order_aid\", \"index\": \"order_index\"})\n",
    ")\n",
    "\n",
    "orders_after_carts_in_sub_sessions = (\n",
    "    orders_of_sub_sessions\n",
    "    # Combine carts and orders of sub sessions\n",
    "    .join(carts_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only orders that happened after carts\n",
    "    .filter(pl.col(\"cart_ts\") < pl.col(\"order_ts\"))\n",
    "    .select([\"sub_session\", \"cart_aid\", \"order_aid\", \"cart_index\", \"order_index\"])\n",
    "    # Weight the cart-to-order relation based in index. Next order has weight 1/1, second 1/2, third 1/3, etc...\n",
    "    .with_columns(weight=(1/(pl.col(\"order_index\")-pl.col(\"cart_index\"))).cast(pl.Float32))\n",
    "    .drop([\"cart_index\", \"order_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each cart-order pair\n",
    "cart_to_order_count = (\n",
    "    orders_after_carts_in_sub_sessions\n",
    "    .group_by([\"cart_aid\", \"order_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each cart\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_carts_total_count = (\n",
    "    orders_after_carts_in_sub_sessions\n",
    "    .group_by(\"cart_aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being added to order after another item has been clicked\n",
    "cart_to_order_matrix = (\n",
    "    cart_to_order_count\n",
    "    .join(aid_carts_total_count, on=\"cart_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"cart_aid\", \"order_aid\"])\n",
    ")\n",
    "\n",
    "print(cart_to_order_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", cart_to_order_matrix.select(\"cart_aid\").n_unique())\n",
    "print(\"Total probability:\", cart_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "cart_to_order_matrix.write_csv(\"./cart_to_order_matrix_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cart to order from whole sessions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (28_135_990, 3)\n",
      "┌─────────┬──────────┬─────────────┐\n",
      "│ aid     ┆ next_aid ┆ probability │\n",
      "│ ---     ┆ ---      ┆ ---         │\n",
      "│ u32     ┆ u32      ┆ f32         │\n",
      "╞═════════╪══════════╪═════════════╡\n",
      "│ 3       ┆ 3        ┆ 0.366832    │\n",
      "│ 3       ┆ 22107    ┆ 0.00957     │\n",
      "│ 3       ┆ 61428    ┆ 0.00957     │\n",
      "│ 3       ┆ 67776    ┆ 0.00957     │\n",
      "│ 3       ┆ 68426    ┆ 0.00319     │\n",
      "│ …       ┆ …        ┆ …           │\n",
      "│ 1855601 ┆ 1486834  ┆ 0.062241    │\n",
      "│ 1855601 ┆ 1566830  ┆ 0.062241    │\n",
      "│ 1855601 ┆ 1700846  ┆ 0.082988    │\n",
      "│ 1855601 ┆ 1712873  ┆ 0.062241    │\n",
      "│ 1855601 ┆ 1786336  ┆ 0.062241    │\n",
      "└─────────┴──────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "carts_and_orders_df = (\n",
    "    df\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    .filter(pl.col(\"type\") != \"clicks\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .drop(\"ts\")\n",
    "    # Make groups based on types and then make sure that session boundaries are not crossed\n",
    "    .with_columns(group=(pl.col(\"type\") == \"orders\").rle_id().cast(pl.UInt32))\n",
    "    .with_columns(group=pl.struct(\"session\", \"group\").rle_id())\n",
    "    # Remove multiple carts and orders in the same group\n",
    "    .unique()\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "cart_to_order = (\n",
    "     carts_and_orders_df\n",
    "    .join(carts_and_orders_df.rename({\"group\": \"next_group\", \"aid\": \"next_aid\", \"type\": \"next_type\" }), on=\"session\", how=\"inner\")\n",
    "    .filter(pl.col(\"group\") < pl.col(\"next_group\"))\n",
    "    .filter((pl.col(\"type\") == \"carts\") & (pl.col(\"next_type\") == \"orders\"))\n",
    "    .drop([\"type\", \"next_type\"])\n",
    "    # time decay\n",
    "    .with_columns(weight = (1 / (pl.col(\"next_group\") - pl.col(\"group\"))).cast(pl.Float32))\n",
    "    .drop([\"group\", \"next_group\"])\n",
    ")\n",
    "\n",
    "cart_to_order_count = (\n",
    "    cart_to_order\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    # time decay\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    "    # no time decay\n",
    "    # .agg(pl.len().alias(\"count\"))\n",
    "    # .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "cart_aid_total_count = (\n",
    "    cart_to_order_count\n",
    "    .group_by(\"aid\")\n",
    "    # time decay\n",
    "    .agg(pl.col(\"weighted_count\").sum().alias(\"weighted_total_count\"))\n",
    "    # no time decay\n",
    "    # .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    # .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "cart_to_order_matrix_df = (\n",
    "    cart_to_order_count\n",
    "    .join(cart_aid_total_count, on=\"aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(cart_to_order_matrix_df)\n",
    "\n",
    "# save to csv\n",
    "cart_to_order_matrix_df.write_csv(\"./cart_to_order_matrix_whole_sessions_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Order to click matrix\n",
    "Order to click matrix is defined as the probabilities of other aids being clicked immediately after an aid is ordered."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get orders and clicks of sub sessions\n",
    "orders_of_sub_sessions = get_orders_of_sub_session(sub_sessions)\n",
    "\n",
    "clicks_of_sub_sessions = get_clicks_of_sub_session(sub_sessions)\n",
    "\n",
    "clicks_after_orders_in_sub_sessions = (\n",
    "    clicks_of_sub_sessions\n",
    "    # Combine orders and clicks of sub sessions\n",
    "    .join(orders_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only clicks that happened after orders\n",
    "    .filter(pl.col(\"order_ts\") < pl.col(\"click_ts\"))\n",
    "    .select([\"sub_session\", \"order_aid\", \"click_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same order to click events there are\n",
    "order_to_click_count = (\n",
    "    clicks_after_orders_in_sub_sessions\n",
    "    .group_by([\"order_aid\", \"click_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the orders for each aid\n",
    "aid_orders_total_count = (\n",
    "    order_to_click_count\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being clicked immediately after another item has been ordered\n",
    "order_to_click_matrix = (\n",
    "    order_to_click_count\n",
    "    .join(aid_orders_total_count, on=\"order_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"order_aid\", \"click_aid\"])\n",
    ")\n",
    "\n",
    "print(order_to_click_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", order_to_click_matrix.select(\"order_aid\").n_unique())\n",
    "print(\"Total probability:\", order_to_click_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "order_to_click_matrix.write_csv(\"./order_to_click_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Order to cart matrix\n",
    "Order to cart matrix is defined as the probabilities of other aids being added to cart later in the same sub session where an aid is ordered."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get orders and carts of sub sessions\n",
    "orders_of_sub_sessions = get_orders_of_sub_session(sub_sessions)\n",
    "\n",
    "carts_of_sub_sessions = get_carts_of_sub_session(sub_sessions)\n",
    "\n",
    "carts_after_orders_in_sub_sessions = (\n",
    "    carts_of_sub_sessions\n",
    "    # Combine orders and carts of sub sessions\n",
    "    .join(orders_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only carts that happened after orders\n",
    "    .filter(pl.col(\"order_ts\") < pl.col(\"cart_ts\"))\n",
    "    .select([\"sub_session\", \"order_aid\", \"cart_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same order to cart events there are\n",
    "order_to_cart_count = (\n",
    "    carts_after_orders_in_sub_sessions\n",
    "    .group_by([\"order_aid\", \"cart_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the orders for each aid\n",
    "aid_orders_total_count = (\n",
    "    order_to_cart_count\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being added to cart after another item has been ordered\n",
    "order_to_cart_matrix = (\n",
    "    order_to_cart_count\n",
    "    .join(aid_orders_total_count, on=\"order_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"order_aid\", \"cart_aid\"])\n",
    ")\n",
    "\n",
    "print(order_to_cart_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", order_to_cart_matrix.select(\"order_aid\").n_unique())\n",
    "print(\"Total probability:\", order_to_cart_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "order_to_cart_matrix.write_csv(\"./order_to_cart_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Order to cart with time decay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = (\n",
    "    get_sub_sessions(with_next_event=False)\n",
    "    # Filter out clicks\n",
    "    .filter(pl.col(\"type\") != \"clicks\")\n",
    "    .with_columns(index=(pl.col(\"sub_session\") + (pl.col(\"type\") == \"cart\").cum_sum()))\n",
    ")\n",
    "\n",
    "# Get clicks and orders of sub sessions\n",
    "orders_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"orders\")\n",
    "    .rename({\"ts\": \"order_ts\", \"aid\": \"order_aid\", \"index\": \"order_index\"})\n",
    ")\n",
    "\n",
    "carts_of_sub_sessions = (\n",
    "    sub_sessions\n",
    "    .filter(pl.col(\"type\") == \"carts\")\n",
    "    .rename({\"ts\": \"cart_ts\", \"aid\": \"cart_aid\", \"index\": \"cart_index\"})\n",
    ")\n",
    "\n",
    "carts_after_orders_in_sub_sessions = (\n",
    "    carts_of_sub_sessions\n",
    "    # Combine carts and orders of sub sessions\n",
    "    .join(orders_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    # Keep only orders that happened after carts\n",
    "    .filter(pl.col(\"order_ts\") < pl.col(\"cart_ts\"))\n",
    "    .select([\"sub_session\", \"cart_aid\", \"order_aid\", \"cart_index\", \"order_index\"])\n",
    "    # Weight the cart-to-order relation based in index. Next order has weight 1/1, second 1/2, third 1/3, etc...\n",
    "    .with_columns(weight=(1/(pl.col(\"cart_index\")-pl.col(\"order_index\"))).cast(pl.Float32))\n",
    "    .drop([\"cart_index\", \"order_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each order-cart pair\n",
    "order_to_cart_count = (\n",
    "    orders_after_carts_in_sub_sessions\n",
    "    .group_by([\"order_aid\", \"cart_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each cart\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_orders_total_count = (\n",
    "    orders_after_carts_in_sub_sessions\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being added to cart after another item has been ordered\n",
    "order_to_cart_matrix = (\n",
    "    order_to_cart_count\n",
    "    .join(aid_orders_total_count, on=\"order_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"order_aid\", \"cart_aid\"])\n",
    ")\n",
    "\n",
    "print(order_to_cart_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", order_to_cart_matrix.select(\"order_aid\").n_unique())\n",
    "print(\"Total probability:\", order_to_cart_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "order_to_cart_matrix.write_csv(\"./order_to_cart_matrix_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Order to order matrix\n",
    "Order to order matrix is defined as the probabilities of other aids being ordered later in the same sub session where an aid is ordered."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "# Get orders of sub sessions\n",
    "orders_of_sub_sessions = get_orders_of_sub_session(sub_sessions)\n",
    "\n",
    "next_orders_of_sub_sessions = (\n",
    "    orders_of_sub_sessions\n",
    "    .rename({\"order_ts\": \"next_order_ts\", \"order_aid\": \"next_order_aid\"})\n",
    ")\n",
    "\n",
    "# Find subsequent orders in the same sub session\n",
    "subsequent_orders = (\n",
    "    orders_of_sub_sessions\n",
    "    .join(next_orders_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    .filter(pl.col(\"order_ts\") < pl.col(\"next_order_ts\"))\n",
    "    .select([\"sub_session\", \"order_aid\", \"next_order_aid\"])\n",
    ")\n",
    "\n",
    "# Count how many same order to order events there are\n",
    "subsequent_orders_count = (\n",
    "    subsequent_orders\n",
    "    .group_by([\"order_aid\", \"next_order_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "\n",
    "# Sum all the orders for each aid\n",
    "aid_orders_total_count = (\n",
    "    subsequent_orders_count\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.sum(\"count\").alias(\"total_count\"))\n",
    "    .sort(\"total_count\", descending=True)\n",
    ")\n",
    "\n",
    "# Calculate the probabilities of items being ordered after another item has been ordered\n",
    "order_to_order_matrix = (\n",
    "    subsequent_orders_count\n",
    "    .join(aid_orders_total_count, on=\"order_aid\")\n",
    "    .with_columns(\n",
    "        probability = pl.col(\"count\") / pl.col(\"total_count\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"probability\").cast(pl.Float32))\n",
    "    .drop([\"count\", \"total_count\"])\n",
    "    .sort([\"order_aid\", \"next_order_aid\"])\n",
    ")\n",
    "\n",
    "print(order_to_order_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", order_to_order_matrix.select(\"order_aid\").n_unique())\n",
    "print(\"Total probability:\", order_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "order_to_order_matrix.write_csv(\"./order_to_order_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Order to order with time decay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orders_of_sub_sessions = (\n",
    "    get_sub_sessions(with_next_event=False)\n",
    "    .filter(pl.col(\"type\") == \"orders\")\n",
    "    .with_row_index()\n",
    "    .rename({\"ts\": \"order_ts\", \"aid\": \"order_aid\", \"index\": \"order_index\"})\n",
    ")\n",
    "\n",
    "next_orders_of_sub_sessions = (\n",
    "    orders_of_sub_sessions\n",
    "    .rename({\"order_ts\": \"next_order_ts\", \"order_aid\": \"next_order_aid\", \"order_index\": \"next_order_index\"})\n",
    ")\n",
    "\n",
    "# Find subsequent orders in the same sub session\n",
    "subsequent_orders = (\n",
    "    orders_of_sub_sessions\n",
    "    .join(next_orders_of_sub_sessions, on=\"sub_session\", how=\"inner\")\n",
    "    .filter(pl.col(\"order_ts\") < pl.col(\"next_order_ts\"))\n",
    "    .select([\"sub_session\", \"order_aid\", \"next_order_aid\", \"order_index\", \"next_order_index\"])\n",
    "    # Weight the order-to-order relation based in index. Next order has weight 1/1, second 1/2, third 1/3, etc...\n",
    "    .with_columns(weight=(1/(pl.col(\"next_order_index\")-pl.col(\"order_index\"))).cast(pl.Float32))\n",
    "    .drop([\"order_index\", \"next_order_index\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each order-order pair\n",
    "order_to_order_count = (\n",
    "    subsequent_orders\n",
    "    .group_by([\"order_aid\", \"next_order_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each order\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_orders_total_count = (\n",
    "    subsequent_orders\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being ordered after another item has been ordered\n",
    "order_to_order_matrix = (\n",
    "    order_to_order_count\n",
    "    .join(aid_orders_total_count, on=\"order_aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"order_aid\", \"next_order_aid\"])\n",
    ")\n",
    "\n",
    "print(order_to_order_matrix)\n",
    "\n",
    "# Check that probabilities sum to the amount of unique aids\n",
    "print(\"Unique aids:\", order_to_order_matrix.select(\"order_aid\").n_unique())\n",
    "print(\"Total probability:\", order_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to csv\n",
    "order_to_order_matrix.write_csv(\"./order_to_order_matrix_time_decay.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Order to order whole sessions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (18_196_971, 3)\n",
      "┌───────────┬────────────────┬─────────────┐\n",
      "│ order_aid ┆ next_order_aid ┆ probability │\n",
      "│ ---       ┆ ---            ┆ ---         │\n",
      "│ u32       ┆ u32            ┆ f32         │\n",
      "╞═══════════╪════════════════╪═════════════╡\n",
      "│ 1836735   ┆ 1836735        ┆ 1.0         │\n",
      "│ 1835249   ┆ 1740171        ┆ 1.0         │\n",
      "│ 1798478   ┆ 1798478        ┆ 1.0         │\n",
      "│ 1718110   ┆ 1718110        ┆ 1.0         │\n",
      "│ 1673053   ┆ 37525          ┆ 1.0         │\n",
      "│ …         ┆ …              ┆ …           │\n",
      "│ 231487    ┆ 644882         ┆ 0.000001    │\n",
      "│ 846545    ┆ 1094510        ┆ 0.000001    │\n",
      "│ 756588    ┆ 1711735        ┆ 0.000001    │\n",
      "│ 846545    ┆ 444504         ┆ 0.000001    │\n",
      "│ 846545    ┆ 1252537        ┆ 0.000001    │\n",
      "└───────────┴────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "orders_df = (\n",
    "    df\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    .drop(\"ts\")\n",
    "    # Make groups based on types and then make sure that session boundaries are not crossed\n",
    "    .with_columns(group=(pl.col(\"type\") == \"orders\").rle_id().cast(pl.UInt32))\n",
    "    .with_columns(group=pl.struct(\"session\", \"group\").rle_id())\n",
    "    .filter(pl.col(\"type\") == \"orders\")\n",
    "    .drop(\"type\")\n",
    "    # Remove multiple orders in the same group\n",
    "    .unique()\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "order_to_order = (\n",
    "    orders_df\n",
    "    .join(orders_df.rename({\"group\": \"next_group\", \"aid\": \"next_aid\"}), on=\"session\", how=\"inner\")\n",
    "    .filter(pl.col(\"group\") <= pl.col(\"next_group\"))\n",
    "    # Filter out same items ordered multiple times at the same time\n",
    "    .filter((pl.col(\"group\") != pl.col(\"next_group\")) | (pl.col(\"aid\") != pl.col(\"next_aid\")))\n",
    "    # time decay\n",
    "    .with_columns(weight = (1 / (pl.col(\"next_group\") - pl.col(\"group\") + 1)).cast(pl.Float32))\n",
    "    .drop([\"group\", \"next_group\"])\n",
    ")\n",
    "\n",
    "# Sum all the weights for each order-order pair\n",
    "order_to_order_count = (\n",
    "    order_to_order\n",
    "    .group_by([\"aid\", \"next_aid\"])\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_count\"))\n",
    ")\n",
    "\n",
    "# Count total weight for each order\n",
    "# Since we have weights in play we need to count weighed sum instead of count of rows\n",
    "aid_orders_total_count = (\n",
    "    order_to_order\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.col(\"weight\").sum().alias(\"weighted_total_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate the weighted probabilities of items being ordered after another item has been ordered\n",
    "order_to_order_matrix = (\n",
    "    order_to_order_count\n",
    "    .join(aid_orders_total_count, on=\"aid\", how=\"inner\")\n",
    "    .with_columns(\n",
    "        probability = (pl.col(\"weighted_count\") / pl.col(\"weighted_total_count\")).cast(pl.Float32)\n",
    "    )\n",
    "    .drop([\"weighted_count\", \"weighted_total_count\"])\n",
    "    .sort([\"aid\", \"next_aid\"])\n",
    "    .rename({\"aid\": \"order_aid\", \"next_aid\": \"next_order_aid\"})\n",
    "    .sort([\"probability\", \"order_aid\", \"next_order_aid\"], descending=True)\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "print(order_to_order_matrix)\n",
    "order_to_order_matrix.write_csv(\"./order_to_order_matrix_whole_sessions_time_decay.csv\")\n",
    "\n",
    "\n",
    "# # Check that probabilities sum to the amount of unique aids\n",
    "# print(\"Unique aids:\", order_to_order_matrix.select(\"order_aid\").n_unique())\n",
    "# print(\"Total probability:\", order_to_order_matrix.select(\"probability\").sum().select(pl.first()).item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Order incompatibility matrix\n",
    "Order incompatibility matrix is defined as links between items which are often ordered with same items but are never ordered together."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (54_885_019, 3)\n",
      "┌─────────┬────────┬──────────┐\n",
      "│ aid     ┆ type   ┆ group    │\n",
      "│ ---     ┆ ---    ┆ ---      │\n",
      "│ u32     ┆ str    ┆ u32      │\n",
      "╞═════════╪════════╪══════════╡\n",
      "│ 650278  ┆ clicks ┆ 1928806  │\n",
      "│ 1170472 ┆ clicks ┆ 5669514  │\n",
      "│ 120879  ┆ clicks ┆ 6288250  │\n",
      "│ 782195  ┆ clicks ┆ 14680300 │\n",
      "│ 614880  ┆ clicks ┆ 8579638  │\n",
      "│ …       ┆ …      ┆ …        │\n",
      "│ 462368  ┆ clicks ┆ 14932259 │\n",
      "│ 698925  ┆ clicks ┆ 6344663  │\n",
      "│ 1475721 ┆ clicks ┆ 3193570  │\n",
      "│ 1808088 ┆ orders ┆ 6065309  │\n",
      "│ 212895  ┆ clicks ┆ 5200209  │\n",
      "└─────────┴────────┴──────────┘\n",
      "shape: (17_404, 2)\n",
      "┌───────────┬──────────────────────────────┐\n",
      "│ order_aid ┆ context                      │\n",
      "│ ---       ┆ ---                          │\n",
      "│ u32       ┆ list[u32]                    │\n",
      "╞═══════════╪══════════════════════════════╡\n",
      "│ 721998    ┆ [1528260, 721998]            │\n",
      "│ 1344385   ┆ [608780, 979311, … 293028]   │\n",
      "│ 441353    ┆ [139685, 441353]             │\n",
      "│ 1058104   ┆ [1058104, 1380567]           │\n",
      "│ 1328763   ┆ [1717891, 896008, … 477708]  │\n",
      "│ …         ┆ …                            │\n",
      "│ 1453919   ┆ [127843, 1453919]            │\n",
      "│ 1286798   ┆ [1286798, 558137, … 1722051] │\n",
      "│ 202774    ┆ [883073, 226413, … 202774]   │\n",
      "│ 1048246   ┆ [1048246, 803326]            │\n",
      "│ 760241    ┆ [1483508, 1088859, 760241]   │\n",
      "└───────────┴──────────────────────────────┘\n",
      "shape: (12_362_280, 2)\n",
      "┌─────────┬──────────────┐\n",
      "│ aid     ┆ ordered_with │\n",
      "│ ---     ┆ ---          │\n",
      "│ u32     ┆ u32          │\n",
      "╞═════════╪══════════════╡\n",
      "│ 602400  ┆ 589942       │\n",
      "│ 1603774 ┆ 1781092      │\n",
      "│ 490360  ┆ 986762       │\n",
      "│ 638992  ┆ 936609       │\n",
      "│ 1006185 ┆ 1551519      │\n",
      "│ …       ┆ …            │\n",
      "│ 868877  ┆ 62712        │\n",
      "│ 336902  ┆ 143517       │\n",
      "│ 959091  ┆ 243252       │\n",
      "│ 28180   ┆ 1760665      │\n",
      "│ 19703   ┆ 911075       │\n",
      "└─────────┴──────────────┘\n",
      "Start\n",
      "row 500 / 17404\n",
      "row 1000 / 17404\n",
      "row 1500 / 17404\n",
      "row 2000 / 17404\n",
      "row 2500 / 17404\n",
      "row 3000 / 17404\n",
      "row 3500 / 17404\n",
      "row 4000 / 17404\n",
      "row 4500 / 17404\n",
      "row 5000 / 17404\n",
      "row 5500 / 17404\n",
      "row 6000 / 17404\n",
      "row 6500 / 17404\n",
      "row 7000 / 17404\n",
      "row 7500 / 17404\n",
      "row 8000 / 17404\n",
      "row 8500 / 17404\n",
      "row 9000 / 17404\n",
      "row 9500 / 17404\n",
      "row 10000 / 17404\n",
      "row 10500 / 17404\n",
      "row 11000 / 17404\n",
      "row 11500 / 17404\n",
      "row 12000 / 17404\n",
      "row 12500 / 17404\n",
      "row 13000 / 17404\n",
      "row 13500 / 17404\n",
      "row 14000 / 17404\n",
      "row 14500 / 17404\n",
      "row 15000 / 17404\n",
      "row 15500 / 17404\n",
      "row 16000 / 17404\n",
      "row 16500 / 17404\n",
      "row 17000 / 17404\n",
      "Done\n",
      "\n",
      "shape: (38_313, 2)\n",
      "┌─────────┬──────────────────┐\n",
      "│ aid     ┆ incompatible_aid │\n",
      "│ ---     ┆ ---              │\n",
      "│ u32     ┆ u32              │\n",
      "╞═════════╪══════════════════╡\n",
      "│ 344     ┆ 411195           │\n",
      "│ 447     ┆ 139752           │\n",
      "│ 447     ┆ 670006           │\n",
      "│ 447     ┆ 1853288          │\n",
      "│ 787     ┆ 1063368          │\n",
      "│ …       ┆ …                │\n",
      "│ 1854872 ┆ 1823816          │\n",
      "│ 1855016 ┆ 1321282          │\n",
      "│ 1855260 ┆ 873637           │\n",
      "│ 1855339 ┆ 132214           │\n",
      "│ 1855594 ┆ 727265           │\n",
      "└─────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "grouped_clicks_and_orders = (\n",
    "    df\n",
    "    .explode(\"events\")\n",
    "    .unnest(\"events\")\n",
    "    .sort([\"session\", \"ts\"], descending=[False, False])\n",
    "    # Convert ts to seconds and cast to UInt32 to save memory\n",
    "    .with_columns((pl.col(\"ts\")//1000).cast(pl.UInt32))\n",
    "    # .drop(\"ts\")\n",
    "    # Make groups based on types and then make sure that session boundaries are not crossed\n",
    "    .filter(pl.col(\"type\") != \"carts\")\n",
    "    .with_columns(group=(pl.col(\"type\") == \"orders\").rle_id().cast(pl.UInt32))\n",
    "    .with_columns(group=pl.struct(\"session\", \"group\").rle_id())\n",
    "    .with_columns(group=pl.when(pl.col(\"type\") == \"clicks\").then(pl.col(\"group\")+1).otherwise(pl.col(\"group\")))\n",
    "    # Filter out events that happened over 1 hour before the last order of the group\n",
    "    # This is done to form a context for ordered item\n",
    "    .with_columns(max_ts=pl.max(\"ts\").over(\"group\"))\n",
    "    .filter(pl.col(\"ts\") > pl.col(\"max_ts\") - 3600)\n",
    "    .drop([\"session\", \"ts\", \"max_ts\"])\n",
    "    # Keep only unique events\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "print(grouped_clicks_and_orders)\n",
    "\n",
    "# how many times click order pair has to be seen to be considered reliable\n",
    "# Setting the threshold to 5 leaves ~1.4 % of the data removing the long tail\n",
    "click_order_count_threshold = 10\n",
    "# how many times items have to be ordered together to be considered often ordered together\n",
    "ordered_together_count_threshold = 0\n",
    "# how similar the contexts of two items have to be to be considered incompatible\n",
    "context_similarity_threshold = 0.8\n",
    "\n",
    "order_context = (\n",
    "    grouped_clicks_and_orders\n",
    "    .rename({\"aid\": \"click_aid\"})\n",
    "    # Get pairs of items ordered at the same time\n",
    "    .join(grouped_clicks_and_orders.rename({\"aid\": \"order_aid\"}), on=\"group\", how=\"inner\")\n",
    "    .filter((pl.col(\"type\") == \"clicks\") & (pl.col(\"type_right\") == \"orders\"))\n",
    "    .drop([\"type\", \"type_right\", \"group\"])\n",
    "    .group_by([\"click_aid\", \"order_aid\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    # Filter out unreliable relations with too few counts\n",
    "    .filter(pl.col(\"count\") >= click_order_count_threshold)\n",
    "    # Create context for order by joining clicked aids into list\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.col(\"click_aid\").alias(\"context\"), pl.len())\n",
    "    # Filter out orders with only the item itself in context\n",
    "    .filter(pl.col(\"len\") > 1)\n",
    "    .drop(\"len\")\n",
    ")\n",
    "\n",
    "print(order_context)\n",
    "\n",
    "aids_often_ordered_together = (\n",
    "    grouped_clicks_and_orders\n",
    "    .filter(pl.col(\"type\") == \"orders\")\n",
    "    .drop(\"type\")\n",
    "    .join(grouped_clicks_and_orders.filter(pl.col(\"type\") == \"orders\").drop(\"type\").rename({\"aid\": \"ordered_with\"}), on=\"group\", how=\"inner\")\n",
    "    .filter(pl.col(\"aid\") != pl.col(\"ordered_with\"))\n",
    "    .drop(\"group\")\n",
    "    .group_by([\"aid\", \"ordered_with\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    # Filter out unreliable relations with too few counts\n",
    "    .filter(pl.col(\"count\") >= ordered_together_count_threshold)\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "print(aids_often_ordered_together)\n",
    "\n",
    "\n",
    "# Find all incompatible pairs by finding aids that have never been ordered together but have common aids that are often clicked before ordering\n",
    "candidate_incompatible_pairs = []\n",
    "\n",
    "order_context_rows = order_context.rows()\n",
    "len_order_context = len(order_context_rows)\n",
    "print(\"Start\")\n",
    "for index, row in enumerate(order_context.iter_rows()):\n",
    "    if (index+1) % 500 == 0:\n",
    "        print(f\"row {index+1} / {len_order_context}\")\n",
    "\n",
    "    order_aid = row[0]\n",
    "    order_aid_context = row[1]\n",
    "\n",
    "    for click_aid in order_aid_context:\n",
    "        # click_aid_context = order_context.row(by_predicate=(pl.col(\"order_aid\") == click_aid))[1]\n",
    "        click_aid_context = next((row[1] for row in order_context_rows if row[0] == click_aid), None)\n",
    "        if click_aid_context is not None:\n",
    "            # click_aid_context = click_aid_context[0]\n",
    "            similarity = len(set(order_aid_context)) / len(set(order_aid_context).union(click_aid_context))\n",
    "            if similarity > context_similarity_threshold:\n",
    "                new_candidate_incompatible_pairs = [(order_aid, aid) for aid in click_aid_context]\n",
    "                candidate_incompatible_pairs.extend(new_candidate_incompatible_pairs)\n",
    "\n",
    "print(\"Done\")\n",
    "print()\n",
    "\n",
    "incompatible_df = (\n",
    "    pl.DataFrame(\n",
    "        data=candidate_incompatible_pairs,\n",
    "        orient='row',\n",
    "        schema={\"aid\": pl.UInt32, \"incompatible_aid\": pl.UInt32}\n",
    "    )\n",
    "    .filter(pl.col(\"aid\") != pl.col(\"incompatible_aid\"))\n",
    "    .unique()\n",
    "    # Filter out pairs that are too often bought together\n",
    "    .join(aids_often_ordered_together, left_on=[\"aid\", \"incompatible_aid\"], right_on=[\"aid\", \"ordered_with\"], how=\"anti\")\n",
    "    .sort([\"aid\", \"incompatible_aid\"])\n",
    ")\n",
    "print(incompatible_df)\n",
    "\n",
    "# Write to csv\n",
    "incompatible_df.write_csv(\"./incompatible_matrix_10_0_08.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "row 1000\n",
      "row 2000\n",
      "row 3000\n",
      "row 4000\n",
      "Done\n",
      "\n",
      "shape: (23_192, 2)\n",
      "┌─────────┬──────────────────┐\n",
      "│ aid     ┆ incompatible_aid │\n",
      "│ ---     ┆ ---              │\n",
      "│ u32     ┆ u32              │\n",
      "╞═════════╪══════════════════╡\n",
      "│ 1029    ┆ 732479           │\n",
      "│ 1029    ┆ 1205296          │\n",
      "│ 1029    ┆ 1337658          │\n",
      "│ 1414    ┆ 282805           │\n",
      "│ 1414    ┆ 1581568          │\n",
      "│ …       ┆ …                │\n",
      "│ 1854493 ┆ 1316639          │\n",
      "│ 1854493 ┆ 1458929          │\n",
      "│ 1854493 ┆ 1467999          │\n",
      "│ 1854493 ┆ 1554991          │\n",
      "│ 1854493 ┆ 1617987          │\n",
      "└─────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "sub_sessions = get_sub_sessions(with_next_event=False)\n",
    "\n",
    "orders_of_sub_sessions_df = get_orders_of_sub_session(sub_sessions)\n",
    "\n",
    "# Lower threshold for how many times item has to be ordered so that some conclusions can be made\n",
    "lower_threshold = 3\n",
    "# how many times item has to be ordered with another item so that their relation can be considered strong\n",
    "ordered_together_threshold = 15\n",
    "\n",
    "# Filter aids with too few total orders\n",
    "allowed_aids_df = (\n",
    "    orders_of_sub_sessions_df\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .filter(pl.col(\"count\") >= lower_threshold)\n",
    ")\n",
    "\n",
    "# Find all order aid pairs\n",
    "multi_order_sub_sessions_df = (\n",
    "    orders_of_sub_sessions_df\n",
    "    # filter aids with too few orders\n",
    "    .join(allowed_aids_df.select(\"order_aid\"), on=\"order_aid\", how=\"semi\")\n",
    "    # group orders and get unique aids for each order\n",
    "    .group_by(\"sub_session\")\n",
    "    # Remove items ordered multiple times in single session\n",
    "    .agg(pl.col(\"order_aid\").unique())\n",
    "    # filter out orders with only one item left\n",
    "    .filter(pl.col(\"order_aid\").list.len() > 1)\n",
    "    .explode(\"order_aid\")\n",
    ")\n",
    "\n",
    "# Find all order aid pairs and count how many times these pairs have been ordered together\n",
    "# high correlation is used to draw conclusion of incompatibility\n",
    "# low correlation is used to check if the proposed incompatible pairs are not ordered together\n",
    "low_correlation, high_correlation = (\n",
    "    multi_order_sub_sessions_df\n",
    "    .join(multi_order_sub_sessions_df, on=\"sub_session\", how=\"inner\")\n",
    "    .drop(\"sub_session\")\n",
    "    .group_by([\"order_aid\", \"order_aid_right\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .filter(pl.col(\"order_aid\") != pl.col(\"order_aid_right\"))\n",
    "    .sort(\"count\")\n",
    "    # Partition pairs by how many times they have been ordered together\n",
    "    .with_columns(over_threshold = pl.col(\"count\") >= ordered_together_threshold)\n",
    "    .partition_by(\"over_threshold\")\n",
    ")\n",
    "\n",
    "# Combine aid pairs into groups of aids that have been ordered together\n",
    "low_correlation = (\n",
    "    low_correlation\n",
    "    .drop([\"over_threshold\", \"count\"])\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.col(\"order_aid_right\").alias(\"ordered_together_with\"))\n",
    "    .explode(\"ordered_together_with\")\n",
    ")\n",
    "\n",
    "high_correlation = (\n",
    "    high_correlation\n",
    "    .drop([\"over_threshold\", \"count\"])\n",
    "    .group_by(\"order_aid\")\n",
    "    .agg(pl.col(\"order_aid_right\").alias(\"ordered_together_with\"))\n",
    ")\n",
    "\n",
    "# Find all incompatible pairs by finding aids that have never been ordered together but have common aids that they are often ordered with\n",
    "candidate_incompatible_pairs = []\n",
    "\n",
    "print(\"Start\")\n",
    "for index, order_aid in enumerate(high_correlation.select(\"order_aid\").to_numpy().reshape(-1)):\n",
    "    if (index+1) % 1000 == 0:\n",
    "        print(\"row\", index+1)\n",
    "\n",
    "    aids_ordered_together = high_correlation.row(by_predicate=(pl.col(\"order_aid\") == order_aid))[1]\n",
    "    for aid in aids_ordered_together:\n",
    "        candidate_incompatible_aids = high_correlation.row(by_predicate=(pl.col(\"order_aid\") == aid))[1]\n",
    "        new_candidate_incompatible_pairs = [(order_aid, aid) for aid in candidate_incompatible_aids]\n",
    "        candidate_incompatible_pairs.extend(new_candidate_incompatible_pairs)\n",
    "\n",
    "print(\"Done\")\n",
    "print()\n",
    "\n",
    "\n",
    "incompatible_df = (\n",
    "    pl.DataFrame(\n",
    "        data=candidate_incompatible_pairs,\n",
    "        orient='row',\n",
    "        schema={\"aid\": pl.UInt32, \"incompatible_aid\": pl.UInt32}\n",
    "    )\n",
    "    # Filter out pairs that have low correlation\n",
    "    .join(low_correlation, left_on=[\"aid\", \"incompatible_aid\"], right_on=[\"order_aid\", \"ordered_together_with\"], how=\"anti\")\n",
    "    .filter(pl.col(\"aid\") != pl.col(\"incompatible_aid\"))\n",
    "    .unique()\n",
    "    .sort([\"aid\", \"incompatible_aid\"])\n",
    ")\n",
    "print(incompatible_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "incompatible_df.write_csv(\"./incompatible_matrix_3_15.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incompatible products found for 4931 unique products\n"
     ]
    }
   ],
   "source": [
    "unique_incompatible_count = (\n",
    "    incompatible_df\n",
    "    .select(\"aid\")\n",
    "    .n_unique()\n",
    ")\n",
    "\n",
    "print(f\"Incompatible products found for {unique_incompatible_count} unique products\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Click count matrix\n",
    "Click counts of aids normalized by the maximum click count."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_855_603, 2)\n",
      "┌─────────┬──────────┐\n",
      "│ aid     ┆ weight   │\n",
      "│ ---     ┆ ---      │\n",
      "│ u32     ┆ f32      │\n",
      "╞═════════╪══════════╡\n",
      "│ 0       ┆ 0.000363 │\n",
      "│ 1       ┆ 0.000272 │\n",
      "│ 2       ┆ 0.00014  │\n",
      "│ 3       ┆ 0.019235 │\n",
      "│ 4       ┆ 0.001682 │\n",
      "│ …       ┆ …        │\n",
      "│ 1855598 ┆ 0.000058 │\n",
      "│ 1855599 ┆ 0.000107 │\n",
      "│ 1855600 ┆ 0.000676 │\n",
      "│ 1855601 ┆ 0.000701 │\n",
      "│ 1855602 ┆ 0.000157 │\n",
      "└─────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "click_count_matrix = (\n",
    "    exploded_df\n",
    "    .filter(pl.col(\"type\") == \"clicks\")\n",
    "    .group_by(\"aid\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .with_columns(weight = (pl.col(\"count\") / pl.col(\"count\").max()).cast(pl.Float32))\n",
    "    .drop(\"count\")\n",
    "    .sort(\"aid\")\n",
    ")\n",
    "\n",
    "print(click_count_matrix)\n",
    "\n",
    "click_count_matrix.write_csv(\"./click_count_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
